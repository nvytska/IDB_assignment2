# Assignment 2

This project demonstrates an optimization steps using the **Brazilian E-Commerce Public Dataset by Olist**. The goal was to analyze the top-performing product categories by revenue, delivery performance, and customer reviews.

The process started with a **poorly written, unoptimized query(generated by ChatGPT)**, and evolved into a **clean and efficient SQL query** using optimization techniques: CTEs and indexes.

## âœ… Dataset Used

- **Source**: [Brazilian E-Commerce Public Dataset by Olist (Kaggle)](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
- **Files**: All tables are in `.csv` format
- **Key Tables**:
  - `olist_order_items_dataset.csv`
  - `olist_products_dataset.csv`
  - `olist_orders_dataset.csv`
  - `olist_order_reviews_dataset.csv`
  - `product_category_name_translation.csv`
  - `olist_sellers_dataset.csv`

## ðŸš§ 1. Bad Query 

I started with a **complex unoptimized query**, with:
- Repetitive joins
- No modular structure (CTEs)
- Poor performance (can't even run it

## ðŸ”§ 2. Optimization with CTEs

I refactored the query using Common Table Expressions (CTEs) to:
- Rewrite logic into readable steps
- Pre-filter product categories with >1000 orders (makes sence, because we want to see only high-performable categories)
- Join only once, cleanly, on filtered categories

Effect:
1. Improved readability
2. Reduced repeated joins

## ðŸš€ 3. Optimization with Indexes

To further optimize performance, I created indexes on columns used in:
- Joins (order_id, product_id, seller_id)
- Filters (order_status, product_category_name)

Effect:
1. Dramatically faster join and filter operations
2. Reduced query execution time on large datasets




